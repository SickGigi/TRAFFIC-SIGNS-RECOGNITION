{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is training a Convolutional Neural Network (CNN) to perform an image classification task. The model is built using PyTorch and is trained on a custom dataset of images. Here's a simple explanation of each part:\n",
    "\n",
    "1. Import Libraries: Importing necessary libraries and modules for data manipulation, model creation, and training.\n",
    "\n",
    "2. Parameters: Setting various parameters such as path of data, number of epochs for training, batch size, dimensions of images, test and validation ratios etc.\n",
    "\n",
    "3. Load and split data: The load_data() function loads the images from the given path and assigns them their respective class labels. It splits the data into training, validation, and testing datasets using sklearn's train_test_split().\n",
    "\n",
    "4. Create PyTorch Dataset: The MyDataset class creates a custom PyTorch dataset. It overrides the __getitem__() and __len__() methods for indexing and returning the length of the dataset, respectively. If any transformations are provided, they are applied to the images.\n",
    "\n",
    "5. Data Augmentation: Defines transformations for data augmentation, which are applied to the images in the dataset for better model generalization. The transformations include converting to PIL image, applying random horizontal flips and rotations, and then converting to a tensor.\n",
    "\n",
    "6. CNN Model: Defines a CNN architecture (Net class) with two convolutional layers followed by max pooling, and two fully connected layers for the classification. The forward() method defines the forward pass for the model.\n",
    "\n",
    "7. Training Function: The train_model() function performs the model training. It takes the model, loss function, optimizer, data loaders, and number of epochs as arguments. It also evaluates the model's performance on the validation dataset after each epoch and prints the loss and accuracy.\n",
    "\n",
    "8. Load the Data: The data is loaded and split into training, validation, and test sets by calling the load_data() function.\n",
    "\n",
    "9. Create DataLoaders: PyTorch DataLoader objects are created for the training, validation, and test datasets. These are used to feed data into the model in batches during training and evaluation.\n",
    "\n",
    "10. Create the Model: An instance of the defined CNN model (Net) is created.\n",
    "\n",
    "11. Loss Function and Optimizer: CrossEntropyLoss is used as the loss function, which is suitable for multi-class classification tasks. The Adam optimizer is used to update the model parameters.\n",
    "\n",
    "12. Train the Model: The model is trained using the defined training function (train_model()).\n",
    "\n",
    "13. Save the Model: The trained model parameters are saved to a file (\"trained_model.pt\") for later use or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###################################################################\n",
    "\n",
    "# Parameters\n",
    "path = \"C:/Users/elvis/OneDrive - uniroma1.it/TRAFFIC SIGNS RECOGNITION/dataset\"\n",
    "labelFile = \"C:/Users/elvis/OneDrive - uniroma1.it/TRAFFIC SIGNS RECOGNITION/Traffic-signs-recognition-PyTorch/labels.csv\"\n",
    "batch_size_val = 50\n",
    "steps_per_epoch_val = 2000\n",
    "epochs_val = 30\n",
    "imageDimesions = (32,32,3)\n",
    "testRatio = 0.2\n",
    "validationRatio = 0.2\n",
    "\n",
    "# Load and split the data\n",
    "def load_data():\n",
    "    images = []\n",
    "    classNo = []\n",
    "    myList = os.listdir(path)\n",
    "    print(\"Total Classes Detected:\", len(myList))\n",
    "    noOfClasses = len(myList)\n",
    "    print(\"Importing Classes.....\")\n",
    "    for x in range(len(myList)):\n",
    "        myPicList = os.listdir(path + \"/\" + str(x))\n",
    "        for y in myPicList:\n",
    "            curImg = cv2.imread(path + \"/\" + str(x) + \"/\" + y)\n",
    "            images.append(curImg)\n",
    "            classNo.append(x)\n",
    "        print(x, end=\" \")\n",
    "    print(\" \")\n",
    "\n",
    "    images = np.array(images)\n",
    "    classNo = np.array(classNo)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, classNo, test_size=testRatio)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validationRatio)\n",
    "\n",
    "    return X_train, X_validation, X_test, y_train, y_validation, y_test, noOfClasses\n",
    "\n",
    "# Create PyTorch Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Data Augmentation\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(), # convert numpy array to PILImage\n",
    "    transforms.RandomHorizontalFlip(),  # randomly flip images (data augmentation) during training time only\n",
    "    transforms.RandomRotation(10), # randomly rotate images (data augmentation) during training time only\n",
    "    transforms.ToTensor(), # convert numpy array to tensor\n",
    "])\n",
    "\n",
    "# We are random flipping and rotating the images to make our model more robust to different variations of the same image (data augmentation)\n",
    "\n",
    "# CNN Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 60, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(60, 30, 5)\n",
    "        self.fc1 = nn.Linear(30 * 5 * 5, 500)\n",
    "        self.fc2 = nn.Linear(500, noOfClasses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 30 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                images, labels = data\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        print('Epoch: {}, Loss: {:.5f}, Accuracy: {:.5f}%'.format(epoch+1, val_loss/total, (correct/total)*100))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Load the data\n",
    "X_train, X_validation, X_test, y_train, y_validation, y_test, noOfClasses = load_data()\n",
    "\n",
    "# Create DataLoaders\n",
    "train_data = MyDataset(X_train, y_train, data_transform)\n",
    "val_data = MyDataset(X_validation, y_validation, data_transform)\n",
    "test_data = MyDataset(X_test, y_test, data_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size_val, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size_val, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size_val, shuffle=False)\n",
    "\n",
    "# Create the model\n",
    "model = Net()\n",
    "\n",
    "# Define a Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "model = train_model(model, criterion, optimizer, train_loader, val_loader, epochs=epochs_val) # 30 epochs\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"trained_model.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is capturing a live video feed from a webcam and using the pretrained Convolutional Neural Network (CNN) model to classify objects in the video feed (traffic signs). Here's a simple explanation of each part:\n",
    "\n",
    "1. Import Libraries: Necessary libraries are imported.\n",
    "\n",
    "2. Configuration Parameters: Sets various parameters such as frame dimensions, brightness, and classification threshold. The font for OpenCV's text output is also set.\n",
    "\n",
    "3. Video Capture Initialization: Initializes a video capture object cap using OpenCV's cv2.VideoCapture(0). This opens the webcam for video capture. Frame width, height, and brightness are set for the video capture object.\n",
    "\n",
    "4. CNN Model: Defines a CNN architecture. The model has two convolutional layers, followed by max pooling, and two fully connected layers.\n",
    "\n",
    "5. Model Loading: Loads the pretrained weights from 'trained_model.pt' into the model and sets it to evaluation mode.\n",
    "\n",
    "6. Transforms: Specifies transformations applied to each frame captured from the video feed before feeding it to the model for inference.\n",
    "\n",
    "7. Class Names Function: The getClassName function maps the class indices to the actual names of the traffic signs.\n",
    "\n",
    "8. Main Loop: The script enters a main loop where it continuously reads frames from the video capture object and processes them:\n",
    "\n",
    "    - First, the frame is converted from BGR to RGB.\n",
    "    - Then, the frame is transformed (resized, converted to tensor, and normalized) and an additional dimension is added to the tensor.\n",
    "    - The transformed frame is fed into the model to get predictions.\n",
    "    - If the prediction probability is higher than the threshold, the script puts the class index, class name, and prediction probability as text on the original image using OpenCV's cv2.putText.\n",
    "    - The original image and processed image are displayed in two separate OpenCV windows.\n",
    "    - The loop is broken when the 'q' key is pressed.\n",
    "    \n",
    "9. Clean Up: After the main loop is exited, the video capture is released and all OpenCV windows are destroyed.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATTEMPT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "###################################################################\n",
    "\n",
    "frameWidth = 640\n",
    "frameHeight = 480\n",
    "brightness = 180\n",
    "threshold = 0.40\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, frameWidth)\n",
    "cap.set(4, frameHeight)\n",
    "cap.set(25, brightness)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, no_of_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 60, 5) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(60, 30, 5)\n",
    "        self.fc1 = nn.Linear(30 * 5 * 5, 500)\n",
    "        self.fc2 = nn.Linear(500, no_of_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 30 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net(43)\n",
    "model.load_state_dict(torch.load(\"C:/Users/elvis/OneDrive - uniroma1.it/TRAFFIC SIGNS RECOGNITION/Traffic-signs-recognition-PyTorch/trained_model_V2.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Define the transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Add normalization here.\n",
    "])\n",
    "\n",
    "def getClassName(classNo):\n",
    "    if   classNo == 0: return 'Speed Limit 20 km/h'\n",
    "    elif classNo == 1: return 'Speed Limit 30 km/h'\n",
    "    elif classNo == 2: return 'Speed Limit 50 km/h'\n",
    "    elif classNo == 3: return 'Speed Limit 60 km/h'\n",
    "    elif classNo == 4: return 'Speed Limit 70 km/h'\n",
    "    elif classNo == 5: return 'Speed Limit 80 km/h'\n",
    "    elif classNo == 6: return 'End of Speed Limit 80 km/h'\n",
    "    elif classNo == 7: return 'Speed Limit 100 km/h'\n",
    "    elif classNo == 8: return 'Speed Limit 120 km/h'\n",
    "    elif classNo == 9: return 'No passing'\n",
    "    elif classNo == 10: return 'No passing for vechiles over 3.5 metric tons'\n",
    "    elif classNo == 11: return 'Right-of-way at the next intersection'\n",
    "    elif classNo == 12: return 'Priority road'\n",
    "    elif classNo == 13: return 'Yield'\n",
    "    elif classNo == 14: return 'Stop'\n",
    "    elif classNo == 15: return 'No vechiles'\n",
    "    elif classNo == 16: return 'Vechiles over 3.5 metric tons prohibited'\n",
    "    elif classNo == 17: return 'No entry'\n",
    "    elif classNo == 18: return 'General caution'\n",
    "    elif classNo == 19: return 'Dangerous curve to the left'\n",
    "    elif classNo == 20: return 'Dangerous curve to the right'\n",
    "    elif classNo == 21: return 'Double curve'\n",
    "    elif classNo == 22: return 'Bumpy road'\n",
    "    elif classNo == 23: return 'Slippery road'\n",
    "    elif classNo == 24: return 'Road narrows on the right'\n",
    "    elif classNo == 25: return 'Road work'\n",
    "    elif classNo == 26: return 'Traffic signals'\n",
    "    elif classNo == 27: return 'Pedestrians'\n",
    "    elif classNo == 28: return 'Children crossing'\n",
    "    elif classNo == 29: return 'Bicycles crossing'\n",
    "    elif classNo == 30: return 'Beware of ice/snow'\n",
    "    elif classNo == 31: return 'Wild animals crossing'\n",
    "    elif classNo == 32: return 'End of all speed and passing limits'\n",
    "    elif classNo == 33: return 'Turn right ahead'\n",
    "    elif classNo == 34: return 'Turn left ahead'\n",
    "    elif classNo == 35: return 'Ahead only'\n",
    "    elif classNo == 36: return 'Go straight or right'\n",
    "    elif classNo == 37: return 'Go straight or left'\n",
    "    elif classNo == 38: return 'Keep right'\n",
    "    elif classNo == 39: return 'Keep left'\n",
    "    elif classNo == 40: return 'Roundabout mandatory'\n",
    "    elif classNo == 41: return 'End of no passing'\n",
    "    elif classNo == 42: return 'End of no passing by vechiles over 3.5 metric tons'\n",
    "\n",
    "while True:\n",
    "    success, imgOrignal = cap.read()\n",
    "    \n",
    "    # Convert from BGR to RGB.\n",
    "    # imgOrignal = cv2.cvtColor(imgOrignal, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img = transform(imgOrignal).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(img)\n",
    "        probabilityValue, classIndex = F.softmax(predictions, dim=1).max(1)\n",
    "        probabilityValue = probabilityValue.item()\n",
    "        classIndex = classIndex.item()\n",
    "\n",
    "    if probabilityValue > threshold:\n",
    "        cv2.putText(imgOrignal,str(classIndex)+\" \"+str(getClassName(classIndex)), (120, 35), font, 0.75, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(imgOrignal, str(round(probabilityValue*100,2) )+\"%\", (180, 75), font, 0.75, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    else:\n",
    "        cv2.putText(imgOrignal,\"No Signal Detected\", (120, 35), font, 0.75, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.namedWindow(\"Result\", cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(\"Result\", imgOrignal)  # Convert back to BGR for display.\n",
    "    cv2.imshow(\"Processed Image\", np.transpose((img.squeeze(0).numpy() * 0.5) + 0.5, (1, 2, 0)))  # Undo normalization for display.\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == ord('q'): # wait for 'q' key to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer-vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
